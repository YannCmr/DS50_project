{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data\n",
    "\n",
    "This notebook aim to split the data in a smaller dataset for training and testing.\n",
    "\n",
    "It aim to avoid biais by :\n",
    "- Stratified Sampling \n",
    "- Perceptual Hashing / Feature Embeddings + kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the path\n",
    "And check if the folder exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER_PATH = \"../\"\n",
    "\n",
    "INPUT_DIR = DATA_FOLDER_PATH + \"00_archive/data/\"\n",
    "OUTPUT_DIR = DATA_FOLDER_PATH + \"00_archive/data_samples/\"\n",
    "\n",
    "file_types = [\"train\", \"test\", \"val\"]\n",
    "subdirectories = [\"Coccidiosis\", \"Healthy\", \"New Castle Disease\", \"Salmonella\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All folder structures are in place.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_and_create_path(verbose = False):\n",
    "    \"\"\"\n",
    "    Check if the input directory structure exists and create the output directory structure if it doesn't.\n",
    "\n",
    "    Parameters:\n",
    "        - verbose (bool): If True, print detailed information about the directory structure.\n",
    "\n",
    "    Return :\n",
    "        - None\n",
    "    \"\"\"\n",
    "    # Check input structure\n",
    "    for file_type in file_types:\n",
    "        input_path = os.path.join(INPUT_DIR, file_type)\n",
    "        if not os.path.isdir(input_path):\n",
    "            raise FileNotFoundError(f\"‚ùå Input directory does not exist: {input_path}\")\n",
    "        if verbose :\n",
    "            print(f\"‚úÖ Found directory: {input_path}\")\n",
    "\n",
    "        for subdirectory in subdirectories:\n",
    "            sub_path = os.path.join( input_path, subdirectory)\n",
    "            if not os.path.isdir(sub_path):\n",
    "                raise FileNotFoundError(f\"‚ùå Subdirectory missing: {sub_path}\")\n",
    "            if verbose :\n",
    "                print(f\"  ‚úÖ Found subdirectory: {sub_path}\")\n",
    "\n",
    "    #  Check/create output directory\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        if verbose :\n",
    "            print(f\"üìÅ Output directory created: {OUTPUT_DIR}\")\n",
    "    else:\n",
    "        if verbose :\n",
    "            print(f\"‚úÖ Output directory already exists: {OUTPUT_DIR}\")\n",
    "\n",
    "    # Create output structure if not exist\n",
    "    for file_type in file_types:\n",
    "        output_path = os.path.join(OUTPUT_DIR, file_type)\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "            if verbose :\n",
    "                print(f\"üìÅ Created output directory: {output_path}\")\n",
    "        else:\n",
    "            if verbose :\n",
    "                print(f\"‚úÖ Output directory already exists: {output_path}\")\n",
    "\n",
    "        for subdirectory in subdirectories:\n",
    "            sub_path = os.path.join(output_path, subdirectory)\n",
    "            if not os.path.exists(sub_path):\n",
    "                os.makedirs(sub_path, exist_ok=True)\n",
    "                if verbose :\n",
    "                    print(f\"üìÅ Created output subdirectory: {sub_path}\")\n",
    "            else:\n",
    "                if verbose :\n",
    "                    print(f\"‚úÖ Output subdirectory already exists: {sub_path}\")\n",
    "\n",
    "    print(f\"‚úÖ All folder structures are in place.\")\n",
    "\n",
    "check_and_create_path(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_directory(directory):\n",
    "    \"\"\"\n",
    "    Remove all files and subdirectories in a given directory.\n",
    "\n",
    "    Parameters:\n",
    "        - directory (str): The path to the directory to be cleaned.\n",
    "\n",
    "    Return :\n",
    "        - None\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "\n",
    "def clean_output_samples():\n",
    "    \"\"\"\n",
    "    Clean the output samples directory by removing all files and subdirectories.\n",
    "\n",
    "    Parameters:\n",
    "        - None\n",
    "\n",
    "    Return :\n",
    "        - None\n",
    "    \"\"\"\n",
    "    for file_type in file_types:\n",
    "        output_path = os.path.join(OUTPUT_DIR, file_type)\n",
    "        for subdirectory in subdirectories:\n",
    "            sub_path = os.path.join(output_path, subdirectory)\n",
    "            clean_directory(sub_path)\n",
    "    print(f\"Cleaned output samples directory: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampled data\n",
    "\n",
    "V1 : basic stratified split per class and per folder using `train_test_split` from `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Total files in train: 400000\n",
      "  Coccidiosis: 100000 files\n",
      "  Healthy: 100000 files\n",
      "  New Castle Disease: 100000 files\n",
      "  Salmonella: 100000 files\n",
      "Total files in test: 70677\n",
      "  Coccidiosis: 18752 files\n",
      "  Healthy: 17412 files\n",
      "  New Castle Disease: 15888 files\n",
      "  Salmonella: 18625 files\n",
      "Total files in val: 40000\n",
      "  Coccidiosis: 10000 files\n",
      "  Healthy: 10000 files\n",
      "  New Castle Disease: 10000 files\n",
      "  Salmonella: 10000 files\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#lets see how much data we have\n",
    "def count_files_in_directory(directory):\n",
    "    \"\"\"\n",
    "    Count the number of files in a given directory.\n",
    "\n",
    "    Parameters:\n",
    "        - directory (str): The path to the directory to be counted.\n",
    "\n",
    "    Return :\n",
    "        - int: The number of files in the directory.\n",
    "    \"\"\"\n",
    "    return sum(len(files) for _, _, files in os.walk(directory))\n",
    "\n",
    "def count_files_in_subdirectories(directory):\n",
    "    \"\"\"\n",
    "    Count the number of files in all subdirectories of a given directory.\n",
    "\n",
    "    Parameters:\n",
    "        - directory (str): The path to the directory to be counted.\n",
    "\n",
    "    Return :\n",
    "        - int: The total number of files in all subdirectories.\n",
    "    \"\"\"\n",
    "    total_files = 0\n",
    "    for subdirectory in subdirectories:\n",
    "        sub_path = os.path.join(directory, subdirectory)\n",
    "        total_files += count_files_in_directory(sub_path)\n",
    "    return total_files\n",
    "\n",
    "def count_files_in_all_directories():\n",
    "    \"\"\"\n",
    "    Count the number of files in all directories and subdirectories.\n",
    "\n",
    "    Parameters:\n",
    "        - None\n",
    "\n",
    "    Return :\n",
    "        - None\n",
    "    \"\"\"\n",
    "    for file_type in file_types:\n",
    "        input_path = os.path.join(INPUT_DIR, file_type)\n",
    "        total_files = count_files_in_subdirectories(input_path)\n",
    "        print(f\"Total files in {file_type}: {total_files}\")\n",
    "        for subdirectory in subdirectories:\n",
    "            sub_path = os.path.join(input_path, subdirectory)\n",
    "            num_files = count_files_in_directory(sub_path)\n",
    "            print(f\"  {subdirectory}: {num_files} files\")\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "count_files_in_all_directories()\n",
    "print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES_PER_CLASS = 1000  # arbitrary number of samples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_count_sample(verbose=False):\n",
    "    \"\"\"\n",
    "    Sample a fixed number of images per class per folder and copy them to the output directory.\n",
    "    \"\"\"\n",
    "    for file_type in file_types:\n",
    "        for subdirectory in subdirectories:\n",
    "            input_path = os.path.join(INPUT_DIR, file_type, subdirectory)\n",
    "            output_path = os.path.join(OUTPUT_DIR, file_type, subdirectory)\n",
    "\n",
    "            # List images\n",
    "            images = [f for f in os.listdir(input_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            if len(images) == 0:\n",
    "                print(f\"No images found in {input_path}\")\n",
    "                continue\n",
    "\n",
    "            # Adjust if fewer images than the sample size\n",
    "            sample_count = min(SAMPLES_PER_CLASS, len(images))\n",
    "            sampled_images = random.sample(images, sample_count)\n",
    "\n",
    "            # Copy sampled images\n",
    "            for img in sampled_images:\n",
    "                src = os.path.join(input_path, img)\n",
    "                dst = os.path.join(output_path, img)\n",
    "                shutil.copy2(src, dst)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"{sample_count} images copied to {output_path}\")\n",
    "\n",
    "    print(\"Fixed-count sampling done!\")\n",
    "\n",
    "# clean_output_samples()\n",
    "# fixed_count_sample(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampled data \n",
    "\n",
    "V2 : ResNet + KMeans Diversity Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_output_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # doit afficher True\n",
    "print(torch.cuda.get_device_name(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "IMAGE_SIZE = 224\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "k_tracking = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_k_for_subdir(subdir, iteration, k):\n",
    "    parts = subdir.split(os.sep)\n",
    "    current = k_tracking\n",
    "    for part in parts:\n",
    "        if part not in current:\n",
    "            current[part] = {}\n",
    "        current = current[part]\n",
    "    current[f\"iteration_{iteration}\"] = k\n",
    "\n",
    "\n",
    "def get_next_iteration(subdir):\n",
    "    parts = subdir.split(os.sep)\n",
    "    current = k_tracking\n",
    "    for part in parts:\n",
    "        if part not in current:\n",
    "            return 1  # First iteration if the path doesn't exist\n",
    "        current = current[part]\n",
    "\n",
    "    # Extract the highest iteration number from the keys\n",
    "    existing = [int(k.split('_')[1]) for k in current.keys() if k.startswith('iteration_')]\n",
    "    return max(existing) + 1 if existing else 1\n",
    "\n",
    "def get_k_tracing(k_tracking):\n",
    "\n",
    "    if os.path.exists(\"best_k_foreach.json\"):\n",
    "        with open(\"best_k_foreach.json\", \"r\") as f:\n",
    "            try:\n",
    "                existing = json.load(f)\n",
    "                k_tracking.update(existing)  # Fusionne sans √©craser\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Fichier JSON invalide, d√©marrage d'un k_tracking vide.\")\n",
    "    return k_tracking\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\UTBM\\INFO4\\07_DS50\\DS50_project\\venvProjetDS50\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\UTBM\\INFO4\\07_DS50\\DS50_project\\venvProjetDS50\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "\n",
    "\n",
    "# Load Pretrained ResNet (remove final classification layer)\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet.fc = torch.nn.Identity()\n",
    "resnet = resnet.to(DEVICE).eval()\n",
    "\n",
    "# Preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "def extract_embedding(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        img_tensor = preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            embedding = resnet(img_tensor).squeeze().cpu().numpy()\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_optimal_k(embeddings, k_range=range(5, 15)):\n",
    "    max_possible_k = min(len(embeddings), max(k_range))\n",
    "    best_k = 5\n",
    "    best_score = -1\n",
    "\n",
    "    for k in range(5, max_possible_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42).fit(embeddings)\n",
    "        score = silhouette_score(embeddings, kmeans.labels_)\n",
    "        if score > best_score:\n",
    "            best_k = k\n",
    "            best_score = score\n",
    "\n",
    "    print(f\"Optimal k found: {best_k} with silhouette score: {best_score:.4f}\")\n",
    "\n",
    "    return best_k, best_score\n",
    "\n",
    "\n",
    "def sample_by_auto_clustering(embeddings, paths, total_samples=300, log_info=None):\n",
    "    k, _ = find_optimal_k(embeddings)\n",
    "    k = min(k, len(embeddings))\n",
    "\n",
    "    if log_info:\n",
    "        log_k_for_subdir(log_info['subdir'], log_info['iteration'], k)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(embeddings)\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    samples_per_cluster = total_samples // k\n",
    "    selected_paths = []\n",
    "\n",
    "    for i in range(k):\n",
    "        indices = [j for j, label in enumerate(labels) if label == i]\n",
    "        cluster_embeddings = [embeddings[j] for j in indices]\n",
    "        cluster_paths = [paths[j] for j in indices]\n",
    "\n",
    "        center = kmeans.cluster_centers_[i]\n",
    "        dists = np.linalg.norm(np.array(cluster_embeddings) - center, axis=1)\n",
    "        sorted_indices = np.argsort(dists)\n",
    "\n",
    "        max_samples = min(samples_per_cluster, len(cluster_paths))\n",
    "\n",
    "        for idx in sorted_indices[:max_samples]:\n",
    "            selected_paths.append(cluster_paths[idx])\n",
    "\n",
    "    return selected_paths\n",
    "\n",
    "\n",
    "# For all subdirectories, sample a fixed number of images per class\n",
    "def diverse_sample_per_subdirectory_all(verbose=False):  \n",
    "    input_root = INPUT_DIR\n",
    "    output_root = OUTPUT_DIR \n",
    "    TOTAL_SAMPLES_PER_FOLDER = SAMPLES_PER_CLASS  # You can adjust this per folder\n",
    "    IMAGE_EXTENSIONS = ('.jpg', '.jpeg', '.png')\n",
    "\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "    print(\"Scanning dataset structure...\")\n",
    "\n",
    "    file_types = [d for d in os.listdir(input_root) if os.path.isdir(os.path.join(input_root, d))]\n",
    "\n",
    "    for file_type in file_types:\n",
    "        file_type_path = os.path.join(input_root, file_type)\n",
    "        subdirectories = [d for d in os.listdir(file_type_path) if os.path.isdir(os.path.join(file_type_path, d))]\n",
    "\n",
    "        for subdirectory in subdirectories:\n",
    "            input_path = os.path.join(file_type_path, subdirectory)\n",
    "            output_path = os.path.join(output_root, file_type, subdirectory)\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "            images = [f for f in os.listdir(input_path) if f.lower().endswith(IMAGE_EXTENSIONS)]\n",
    "            image_paths = [os.path.join(input_path, f) for f in images]\n",
    "\n",
    "            embeddings = []\n",
    "            valid_paths = []\n",
    "\n",
    "            for path in tqdm(image_paths, desc=f\"üîç {file_type}/{subdirectory}\", leave=False):\n",
    "                emb = extract_embedding(path)\n",
    "                if emb is not None:\n",
    "                    embeddings.append(emb)\n",
    "                    valid_paths.append(path)\n",
    "\n",
    "            if len(valid_paths) == 0:\n",
    "                print(f\"No valid images in {file_type}/{subdirectory}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"{file_type}/{subdirectory}: {len(valid_paths)} images, extracting clusters...\")\n",
    "\n",
    "            selected_paths = sample_by_auto_clustering(\n",
    "                embeddings,\n",
    "                valid_paths,\n",
    "                total_samples=min(TOTAL_SAMPLES_PER_FOLDER, len(valid_paths))\n",
    "            )\n",
    "\n",
    "            for src in selected_paths:\n",
    "                dst = os.path.join(output_path, os.path.basename(src))\n",
    "                shutil.copy2(src, dst)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Copied {len(selected_paths)} images to {output_path}\")\n",
    "\n",
    "    print(\"Sampling complete for all subdirectories.\")\n",
    "\n",
    "\n",
    "def diverse_sample_per_subdirectory(subdirs, verbose=False):\n",
    "    input_root = INPUT_DIR\n",
    "    output_root = OUTPUT_DIR\n",
    "    TOTAL_SAMPLES_PER_FOLDER = SAMPLES_PER_CLASS\n",
    "    IMAGE_EXTENSIONS = ('.jpg', '.jpeg', '.png')\n",
    "\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "    print(\"Sampling from specific subdirectories...\")\n",
    "\n",
    "    for relative_subdir in subdirs:\n",
    "        iteration = get_next_iteration(relative_subdir)\n",
    "        input_path = os.path.join(input_root, relative_subdir)\n",
    "        output_path = os.path.join(output_root, relative_subdir)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        if not os.path.isdir(input_path):\n",
    "            print(f\"Skipping non-directory: {input_path}\")\n",
    "            continue\n",
    "\n",
    "        images = [f for f in os.listdir(input_path) if f.lower().endswith(IMAGE_EXTENSIONS)]\n",
    "        image_paths = [os.path.join(input_path, f) for f in images]\n",
    "\n",
    "        embeddings = []\n",
    "        valid_paths = []\n",
    "\n",
    "        for path in tqdm(image_paths, desc=f\"{relative_subdir}\", leave=False):\n",
    "            emb = extract_embedding(path)\n",
    "            if emb is not None:\n",
    "                embeddings.append(emb)\n",
    "                valid_paths.append(path)\n",
    "\n",
    "        if len(valid_paths) == 0:\n",
    "            print(f\"No valid images in {relative_subdir}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"{relative_subdir}: {len(valid_paths)} images, extracting clusters...\")\n",
    "\n",
    "        selected_paths = sample_by_auto_clustering(\n",
    "            embeddings,\n",
    "            valid_paths,\n",
    "            total_samples=min(TOTAL_SAMPLES_PER_FOLDER, len(valid_paths)),\n",
    "            log_info={\"subdir\": relative_subdir, \"iteration\": iteration}\n",
    "        )\n",
    "\n",
    "        for src in selected_paths:\n",
    "            dst = os.path.join(output_path, os.path.basename(src))\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Copied {len(selected_paths)} images to {output_path}\")\n",
    "\n",
    "    with open(\"best_k_foreach.json\", \"w\") as f:\n",
    "        json.dump(k_tracking, f, indent=4)\n",
    "\n",
    "    print(\"Sampling complete.\")\n",
    "\n",
    "\n",
    "all_subdirs = [\n",
    "    \"train/Coccidiosis\",#DONE\n",
    "    \"train/Healthy\",#DONE\n",
    "    \"train/New Castle Disease\", \n",
    "    \"train/Salmonella\", \n",
    "    \"val/Coccidiosis\",#DONE\n",
    "    \"val/Healthy\",  #DONE\n",
    "    \"val/New Castle Disease\",#DONE\n",
    "    \"val/Salmonella\",#DONE\n",
    "    \"test/Coccidiosis\", #DONE\n",
    "    \"test/Healthy\", #DONE\n",
    "    \"test/New Castle Disease\",#DONE\n",
    "    \"test/Salmonella\" #DONE\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation\n",
    "k_tracking = {}\n",
    "k_tracking = get_k_tracing(k_tracking)\n",
    "\n",
    "if not os.path.exists(\"best_k_foreach.json\"):\n",
    "    with open(\"best_k_foreach.json\", \"w\") as f:\n",
    "        json.dump(k_tracking, f, indent=4)\n",
    "        print(\"File created: best_k_foreach.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val/Salmonella': {'iteration_1': 5, 'iteration_2': 5, 'iteration_3': 5},\n",
       " 'val/New Castle Disease': {'iteration_1': 6,\n",
       "  'iteration_2': 6,\n",
       "  'iteration_3': 6},\n",
       " 'val/Healthy': {'iteration_1': 5, 'iteration_2': 5, 'iteration_3': 5},\n",
       " 'val/Coccidiosis': {'iteration_1': 8, 'iteration_2': 8, 'iteration_3': 8},\n",
       " 'test/Salmonella': {'iteration_1': 7, 'iteration_2': 9, 'iteration_3': 7},\n",
       " 'test/New Castle Disease': {'iteration_1': 7,\n",
       "  'iteration_2': 7,\n",
       "  'iteration_3': 6},\n",
       " 'test/Healthy': {'iteration_1': 6, 'iteration_2': 10},\n",
       " 'test/Coccidiosis': {'iteration_1': 10, 'iteration_2': 5},\n",
       " 'train/Salmonella': {'iteration_1': 9},\n",
       " 'train/New Castle Disease': {'iteration_1': 7},\n",
       " 'train/Coccidiosis': {'iteration_1': 5},\n",
       " 'train/Healthy': {'iteration_1': 10}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling from specific subdirectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/New Castle Disease: 100000 images, extracting clusters...\n",
      "Optimal k found: 7 with silhouette score: 0.0515\n",
      "Copied 994 images to ../00_archive/data_samples/train/New Castle Disease\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/Salmonella: 100000 images, extracting clusters...\n",
      "Optimal k found: 9 with silhouette score: 0.0487\n",
      "Copied 999 images to ../00_archive/data_samples/train/Salmonella\n",
      "Sampling complete.\n"
     ]
    }
   ],
   "source": [
    "subdir_to_process = [\"train/New Castle Disease\", \"train/Salmonella\" ]\n",
    "diverse_sample_per_subdirectory(subdir_to_process,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old version :`\n",
    "\n",
    "```py \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# Parameters\n",
    "SAMPLES_PER_CLASS = 30\n",
    "IMAGE_SIZE = 224\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load Pretrained ResNet (remove final classification layer)\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet.fc = torch.nn.Identity()\n",
    "resnet = resnet.to(DEVICE).eval()\n",
    "\n",
    "# Preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def extract_embedding(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        img_tensor = preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            embedding = resnet(img_tensor).squeeze().cpu().numpy()\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def diverse_sample_with_kmeans(verbose=False):\n",
    "    \"\"\"\n",
    "    For each class in each file_type folder, extract embeddings, perform KMeans,\n",
    "    and copy the most diverse images (closest to cluster centers).\n",
    "    \"\"\"\n",
    "    for file_type in file_types:\n",
    "        for subdirectory in subdirectories:\n",
    "            input_path = os.path.join(\"../00_archive/data_samples_old\", file_type, subdirectory)\n",
    "            output_path = os.path.join(OUTPUT_DIR, file_type, subdirectory)\n",
    "\n",
    "            # List images\n",
    "            images = [f for f in os.listdir(input_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            image_paths = [os.path.join(input_path, f) for f in images]\n",
    "\n",
    "            # Extract embeddings\n",
    "            embeddings = []\n",
    "            valid_paths = []\n",
    "\n",
    "            for path in tqdm(image_paths, desc=f\"üîç {file_type}/{subdirectory}\", leave=False):\n",
    "                emb = extract_embedding(path)\n",
    "                if emb is not None:\n",
    "                    embeddings.append(emb)\n",
    "                    valid_paths.append(path)\n",
    "\n",
    "            if len(valid_paths) == 0:\n",
    "                print(f\"No valid images found in {input_path}\")\n",
    "                continue\n",
    "\n",
    "            # KMeans clustering\n",
    "            n_clusters = min(SAMPLES_PER_CLASS, len(valid_paths))\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "            kmeans.fit(embeddings)\n",
    "\n",
    "            # Find image closest to each cluster center\n",
    "            selected_paths = []\n",
    "            for center in kmeans.cluster_centers_:\n",
    "                dists = np.linalg.norm(np.array(embeddings) - center, axis=1)\n",
    "                idx = np.argmin(dists)\n",
    "                selected_paths.append(valid_paths[idx])\n",
    "\n",
    "            # Remove duplicates\n",
    "            selected_paths = list(set(selected_paths))\n",
    "\n",
    "            # Copy selected images\n",
    "            for src in selected_paths:\n",
    "                dst = os.path.join(output_path, os.path.basename(src))\n",
    "                shutil.copy2(src, dst)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"{len(selected_paths)} diverse images copied to {output_path}\")\n",
    "\n",
    "    print(\"üéØ Diversity-based sampling complete!\")\n",
    "\n",
    "# üöÄ Run it\n",
    "diverse_sample_with_kmeans(verbose=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of the resnet embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_embeddings(embeddings, labels=None, method='tsne', title='Embeddings Visualization'):\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=2)\n",
    "    elif method == 'tsne':\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'tsne' or 'pca'.\")\n",
    "\n",
    "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    if labels is not None:\n",
    "        scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "        plt.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "    else:\n",
    "        plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.7)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvProjetDS50",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
